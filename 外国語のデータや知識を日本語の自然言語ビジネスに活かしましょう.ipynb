{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "外国語のデータや知識を日本語の自然言語ビジネスに活かしましょう.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otanet/Recognizing_Textual_Entailment/blob/main/%E5%A4%96%E5%9B%BD%E8%AA%9E%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E3%82%84%E7%9F%A5%E8%AD%98%E3%82%92%E6%97%A5%E6%9C%AC%E8%AA%9E%E3%81%AE%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%83%93%E3%82%B8%E3%83%8D%E3%82%B9%E3%81%AB%E6%B4%BB%E3%81%8B%E3%81%97%E3%81%BE%E3%81%97%E3%82%87%E3%81%86.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm7mirNKWeLV"
      },
      "source": [
        "\n",
        "# **外国語のデータや知識を日本語の自然言語ビジネスに活かしましょう**\n",
        "\n",
        "---\n",
        "\n",
        "YouTubeに内容説明の動画：　https://youtu.be/X5JPdY-iZ1g\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1.   **背景：**\n",
        "\n",
        "* 日本語の自然言語処理は、自由に利用できるリソースがほぼ無いです。\n",
        "\n",
        "* 京都大学のデータリソース：\n",
        "http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?NLPresources\n",
        "\n",
        "* そのほか[国立情報学研究所](https://www.nii.ac.jp/dsc/idr/datalist.html)、各企業など、承認を貰わないと使えないし、組織ではなく、個人の申請は難しいです。（現状に情報を独占し、自分を守ると考える企業が多いです。）\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "2.   **データ無い日本語NLPモデル訓練手法の紹介：**\n",
        "　　\n",
        "* BERTの各言語にわたって有効性が高いことに関連する情報と証拠：\n",
        ">  [The Surprising Cross-Lingual Effectiveness of BERT](https://arxiv.org/pdf/1904.09077.pdf)や、[How multilingual is Multilingual BERT](https://www.aclweb.org/anthology/P19-1493.pdf)など多く論文によって、BERTの各言語にわたって有効性が高いことを証明されました。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=15NYfWTKHTTrvJ0ZBsZiQhGRlOUaVg0nh\"  width=75% >\n",
        "\n",
        "\n",
        "\n",
        "* 言語が違うが、BERTの各言語単語トークンは多次元空間の位置がほぼ同じか、近いです。\n",
        "> Sun = 太陽 = 太阳 = ดวงอาทิตย์ = 태양   (太陽に関して、多言語の太陽トークンEmbeddingが近いです。)\n",
        "\n",
        "* 英語や中国語などのデータセットから多国語のBERTモデルを調整して、日本語の自然言語モデルに利用\n",
        "\n",
        "* より多く外国語のデータリソースから日本語の自然言語処理モデル作成しましょう。\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "3.   **Googleさんから、モデルに多言語連携評価のベンチマーク情報**\n",
        "\n",
        "Cross-lingual TRansfer Evaluation of Multilingual Encoders benchmark\n",
        "\n",
        "https://sites.research.google/xtreme\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7fmC1DQBKYj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "353b8f16-ffba-4bc9-8c97-65d80759d16a"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install wget\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70mxf-R4GqPD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eb73ebad-0687-4104-a2c4-2c27cc15b5f3"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUlJeM5tBdox"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "  \n",
        "  return train_df, test_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52qf_XYpF272"
      },
      "source": [
        "\n",
        "## **仮のビジネスタスク：映画に対し、ユーザーの感情認識**\n",
        "\n",
        "映画に日本語の評価に関して、英語のIMDBデータソース（知識）を利用し、日本語の認識モデルを訓練しましょう。\n",
        "\n",
        "[スタンフォード大学人工知能研究所](https://ai.stanford.edu/)のデータセット [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**もちろん、同じく中国語、韓国語のデータ（知識）を利用し、英語や日本語のビジネス製品に展開することも出来ます。**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6vOfIppRvAF"
      },
      "source": [
        "train, test = download_and_load_datasets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLru44gCRyZc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "024b294b-1e81-4227-eea7-83d421c8549a"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I saw this movie when I was really little. It ...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In 1967, mine workers find the remnants of an ...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(contains slight spoilers)&lt;br /&gt;&lt;br /&gt;It's int...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>As a comic book reader, who still sees myself ...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This is hardly a movie at all, but rather a re...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence sentiment  polarity\n",
              "0  I saw this movie when I was really little. It ...         3         0\n",
              "1  In 1967, mine workers find the remnants of an ...         3         0\n",
              "2  (contains slight spoilers)<br /><br />It's int...         7         1\n",
              "3  As a comic book reader, who still sees myself ...         8         1\n",
              "4  This is hardly a movie at all, but rather a re...        10         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xezAuORwUqdT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b8b428e-e9d5-4b43-e781-63ee34156027"
      },
      "source": [
        "len(train),len(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22AulJvLU0HV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "3626376b-1650-4035-8a27-c8cffeaf2d03"
      },
      "source": [
        "train.loc[0]['sentence']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'I saw this movie when I was really little. It is, by far, one of the strangest movies I have ever seen. Now, normally, I like weird movies, but this was just a bit too much.<br /><br />There\\'s not much of a plot to the movie. If anything, it starts out like Toy Story, where toys come to life, and Raggedy Ann and Andy go on an adventure to rescue their new friend, Babette. From there, craziness ensues. There\\'s the Greedy, the Looneys, a sea monster named Gazooks, and a bunch of pirates singing show tunes, all of which just made the movie weirder. Also, I can\\'t help but feel that Babette is annoying and a bit too whiny. She definitely didn\\'t help the movie.<br /><br />Now, even though I didn\\'t like this movie, there were a few cute parts. I liked the camel\\'s song. Even though it was a song about being lonely, it had a friendly feel to it. Then, there was Sir Leonard. While most of the Looneys were just plain nuts, Sir Leonard was the most interesting and probably the funniest. King Koo Koo was just a little dirtbag that made Dr. Evil look like a serious villain. Also, there was Raggedy Andy\\'s song, No Girl\\'s Toy. It was definitely good song for little boys who wanted to act tough. But, honestly, even these things didn\\'t make the movie any better. (But remember, this is just my perspective.) <br /><br />While I personally wouldn\\'t recommend this movie, even I have to admit, it does have its charming moments. See it if you\\'re interested, but only if you\\'re in the mood for something \"really\" out of the ordinary.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ygKaf4Ts1LC"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification,AdamW,BertConfig\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g35A5NFTHWy0"
      },
      "source": [
        "BERT多国語モデルを利用します。 (104国語)\n",
        "\n",
        "いろいろ国の単語と文字の例：\n",
        "\n",
        " 'ұ', 'Ҳ', 'ҳ', 'Ҷ', 'ҷ', 'Һ', 'һ', 'Ӏ', 'ӑ', 'ӗ', 'Ә', 'ә', 'ӣ', 'Ө', 'ө', 'Ӯ', 'ӯ', 'ӳ', 'Ա', 'Բ', 'Գ', 'Դ', 'Ե', 'Զ', 'Է', 'Ը'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXdA3ZIvVeBE"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcd9S1F0uHj7"
      },
      "source": [
        "# tokenizer.vocab.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E_nBXX8urLa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "351ecc4b-477a-4bdd-af3f-6597adf8d910"
      },
      "source": [
        "tokenizer.vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119547"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbe_5Vc6Hywx"
      },
      "source": [
        "もちろん、日本語の文字が入っています。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNzOv-bevBN3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "720e6ba3-1af6-42d7-e172-dc9559aa09f9"
      },
      "source": [
        "tokenizer.tokenize('意味のないアラート発動だった。')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['意', '味', 'のない', '##ア', '##ラー', '##ト', '発', '動', 'だった', '。']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKRggvE_vWYM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "948e6cb3-5f82-41e1-fbd1-60e74e72f576"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('意味のないアラート発動だった。'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3906, 2824, 82181, 18226, 29004, 13913, 5712, 2621, 19101, 1882]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwQ5iIo2virL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2288679f-bd8a-48ea-e508-6ba3ff11200c"
      },
      "source": [
        "tokenizer.encode('意味のないアラート発動だった。', add_special_tokens=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 3906, 2824, 82181, 18226, 29004, 13913, 5712, 2621, 19101, 1882, 102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV00wfClIlak"
      },
      "source": [
        "## **IMDBのデータ前処理**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9K8mfnxCUqd"
      },
      "source": [
        "sentences = train.sentence.values\n",
        "labels = train.polarity.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXUCYOVmCceX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "d6e5176d-9c2b-472a-ca92-f55cf49a70ef"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 256,          \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,  \n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "    input_ids.append(encoded_dict['input_ids'])    \n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  I saw this movie when I was really little. It is, by far, one of the strangest movies I have ever seen. Now, normally, I like weird movies, but this was just a bit too much.<br /><br />There's not much of a plot to the movie. If anything, it starts out like Toy Story, where toys come to life, and Raggedy Ann and Andy go on an adventure to rescue their new friend, Babette. From there, craziness ensues. There's the Greedy, the Looneys, a sea monster named Gazooks, and a bunch of pirates singing show tunes, all of which just made the movie weirder. Also, I can't help but feel that Babette is annoying and a bit too whiny. She definitely didn't help the movie.<br /><br />Now, even though I didn't like this movie, there were a few cute parts. I liked the camel's song. Even though it was a song about being lonely, it had a friendly feel to it. Then, there was Sir Leonard. While most of the Looneys were just plain nuts, Sir Leonard was the most interesting and probably the funniest. King Koo Koo was just a little dirtbag that made Dr. Evil look like a serious villain. Also, there was Raggedy Andy's song, No Girl's Toy. It was definitely good song for little boys who wanted to act tough. But, honestly, even these things didn't make the movie any better. (But remember, this is just my perspective.) <br /><br />While I personally wouldn't recommend this movie, even I have to admit, it does have its charming moments. See it if you're interested, but only if you're in the mood for something \"really\" out of the ordinary.\n",
            "Token IDs: tensor([   101,    146,  17112,  10531,  18379,  10841,    146,  10134,  30181,\n",
            "         16745,    119,  10377,  10124,    117,  10155,  13301,    117,  10464,\n",
            "         10108,  10105,  93309,  10562,  39129,    146,  10529,  17038,  15652,\n",
            "           119,  17121,    117,  48252,    117,    146,  11850,  86981,  12023,\n",
            "         39129,    117,  10473,  10531,  10134,  12820,    169,  17684,  16683,\n",
            "         13172,    119,    133,  33989,    120,    135,    133,  33989,    120,\n",
            "           135,  11723,    112,    187,  10472,  13172,  10108,    169,  32473,\n",
            "         10114,  10105,  18379,    119,  14535,  42819,    117,  10271,  33039,\n",
            "         10950,  11850,  59533,  14656,    117,  10940,  99584,  10107,  10678,\n",
            "         10114,  12103,    117,  10111,  38571,  91518,  10157,  15879,  10111,\n",
            "         16802,  11783,  10135,  10151,  67865,  10114,  48022,  10455,  10751,\n",
            "         20104,    117,  94052,  12131,    119,  12222,  11155,    117,    171,\n",
            "         29948,  90337,  55683,  19031,    119,  11723,    112,    187,  10105,\n",
            "           144,  29711,  12355,    117,  10105,  13069,  87185,  10107,    117,\n",
            "           169,  14931,  76343,  12038,  69699,  72713,  11676,    117,  10111,\n",
            "           169,  59230,  10269,  10108,  88533,  34746,  11897,  91695,  10107,\n",
            "           117,  10435,  10108,  10319,  12820,  11019,  10105,  18379,  86981,\n",
            "         72016,    119,  20593,    117,    146,  10944,    112,    188,  15217,\n",
            "         10473,  38008,  10189,  94052,  12131,  10124,  11671,  40018,  10111,\n",
            "           169,  17684,  16683,    191,  14383,  10157,    119,  11149, 100745,\n",
            "        100240,  10454,  34420,    112,    188,  15217,  10105,  18379,    119,\n",
            "           133,  33989,    120,    135,    133,  33989,    120,    135,  17121,\n",
            "           117,  13246,  15102,    146,  34420,    112,    188,  11850,  10531,\n",
            "         18379,    117,  11155,  10309,    169,  13824,  21610,  10112,  15569,\n",
            "           119,    146,  11850,  10162,  10105,  13383,  10161,    112,    187,\n",
            "         12011,    119,  28140,  15102,  10271,  10134,    169,  12011,  10978,\n",
            "         11223,  10406,  17608,  10157,    117,  10271,  10374,    169,  43941,\n",
            "         38008,  10114,  10271,    119,  26467,    117,  11155,  10134,  12852,\n",
            "         20318,    119,  14600,    102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okspHz7qGCbj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a7a80f19-08e2-4b05-fff3-0f8d7bfe9302"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22,500 training samples\n",
            "2,500 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQzjutbHGT4I"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#batch_size = 32\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbiNbT5CIy7D"
      },
      "source": [
        "## **英語のIMDBデータセットから、多国語NLPモデルに訓練する。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTukYSmWGcmv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3eecb650-43b7-4acb-f190-da0ba453d5fe"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfbU505HHXIx"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtwB8lj3Hji1"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "# epochs = 2\n",
        "epochs = 1\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-Tk4KN4HnzF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjmIw4i4HtFl"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctnAJ_07ITdD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5851a034-d8f1-436f-9fca-1ba0733c73e4"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    # Training\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    40  of  2,813.    Elapsed: 0:00:09.\n",
            "  Batch    80  of  2,813.    Elapsed: 0:00:18.\n",
            "  Batch   120  of  2,813.    Elapsed: 0:00:28.\n",
            "  Batch   160  of  2,813.    Elapsed: 0:00:37.\n",
            "  Batch   200  of  2,813.    Elapsed: 0:00:46.\n",
            "  Batch   240  of  2,813.    Elapsed: 0:00:55.\n",
            "  Batch   280  of  2,813.    Elapsed: 0:01:04.\n",
            "  Batch   320  of  2,813.    Elapsed: 0:01:13.\n",
            "  Batch   360  of  2,813.    Elapsed: 0:01:23.\n",
            "  Batch   400  of  2,813.    Elapsed: 0:01:32.\n",
            "  Batch   440  of  2,813.    Elapsed: 0:01:41.\n",
            "  Batch   480  of  2,813.    Elapsed: 0:01:50.\n",
            "  Batch   520  of  2,813.    Elapsed: 0:01:59.\n",
            "  Batch   560  of  2,813.    Elapsed: 0:02:08.\n",
            "  Batch   600  of  2,813.    Elapsed: 0:02:18.\n",
            "  Batch   640  of  2,813.    Elapsed: 0:02:27.\n",
            "  Batch   680  of  2,813.    Elapsed: 0:02:36.\n",
            "  Batch   720  of  2,813.    Elapsed: 0:02:45.\n",
            "  Batch   760  of  2,813.    Elapsed: 0:02:54.\n",
            "  Batch   800  of  2,813.    Elapsed: 0:03:03.\n",
            "  Batch   840  of  2,813.    Elapsed: 0:03:13.\n",
            "  Batch   880  of  2,813.    Elapsed: 0:03:22.\n",
            "  Batch   920  of  2,813.    Elapsed: 0:03:31.\n",
            "  Batch   960  of  2,813.    Elapsed: 0:03:40.\n",
            "  Batch 1,000  of  2,813.    Elapsed: 0:03:49.\n",
            "  Batch 1,040  of  2,813.    Elapsed: 0:03:58.\n",
            "  Batch 1,080  of  2,813.    Elapsed: 0:04:08.\n",
            "  Batch 1,120  of  2,813.    Elapsed: 0:04:17.\n",
            "  Batch 1,160  of  2,813.    Elapsed: 0:04:26.\n",
            "  Batch 1,200  of  2,813.    Elapsed: 0:04:35.\n",
            "  Batch 1,240  of  2,813.    Elapsed: 0:04:44.\n",
            "  Batch 1,280  of  2,813.    Elapsed: 0:04:54.\n",
            "  Batch 1,320  of  2,813.    Elapsed: 0:05:03.\n",
            "  Batch 1,360  of  2,813.    Elapsed: 0:05:12.\n",
            "  Batch 1,400  of  2,813.    Elapsed: 0:05:21.\n",
            "  Batch 1,440  of  2,813.    Elapsed: 0:05:30.\n",
            "  Batch 1,480  of  2,813.    Elapsed: 0:05:40.\n",
            "  Batch 1,520  of  2,813.    Elapsed: 0:05:49.\n",
            "  Batch 1,560  of  2,813.    Elapsed: 0:05:58.\n",
            "  Batch 1,600  of  2,813.    Elapsed: 0:06:07.\n",
            "  Batch 1,640  of  2,813.    Elapsed: 0:06:16.\n",
            "  Batch 1,680  of  2,813.    Elapsed: 0:06:25.\n",
            "  Batch 1,720  of  2,813.    Elapsed: 0:06:35.\n",
            "  Batch 1,760  of  2,813.    Elapsed: 0:06:44.\n",
            "  Batch 1,800  of  2,813.    Elapsed: 0:06:53.\n",
            "  Batch 1,840  of  2,813.    Elapsed: 0:07:02.\n",
            "  Batch 1,880  of  2,813.    Elapsed: 0:07:11.\n",
            "  Batch 1,920  of  2,813.    Elapsed: 0:07:21.\n",
            "  Batch 1,960  of  2,813.    Elapsed: 0:07:30.\n",
            "  Batch 2,000  of  2,813.    Elapsed: 0:07:39.\n",
            "  Batch 2,040  of  2,813.    Elapsed: 0:07:48.\n",
            "  Batch 2,080  of  2,813.    Elapsed: 0:07:57.\n",
            "  Batch 2,120  of  2,813.    Elapsed: 0:08:06.\n",
            "  Batch 2,160  of  2,813.    Elapsed: 0:08:16.\n",
            "  Batch 2,200  of  2,813.    Elapsed: 0:08:25.\n",
            "  Batch 2,240  of  2,813.    Elapsed: 0:08:34.\n",
            "  Batch 2,280  of  2,813.    Elapsed: 0:08:43.\n",
            "  Batch 2,320  of  2,813.    Elapsed: 0:08:52.\n",
            "  Batch 2,360  of  2,813.    Elapsed: 0:09:01.\n",
            "  Batch 2,400  of  2,813.    Elapsed: 0:09:11.\n",
            "  Batch 2,440  of  2,813.    Elapsed: 0:09:20.\n",
            "  Batch 2,480  of  2,813.    Elapsed: 0:09:29.\n",
            "  Batch 2,520  of  2,813.    Elapsed: 0:09:38.\n",
            "  Batch 2,560  of  2,813.    Elapsed: 0:09:47.\n",
            "  Batch 2,600  of  2,813.    Elapsed: 0:09:56.\n",
            "  Batch 2,640  of  2,813.    Elapsed: 0:10:06.\n",
            "  Batch 2,680  of  2,813.    Elapsed: 0:10:15.\n",
            "  Batch 2,720  of  2,813.    Elapsed: 0:10:24.\n",
            "  Batch 2,760  of  2,813.    Elapsed: 0:10:33.\n",
            "  Batch 2,800  of  2,813.    Elapsed: 0:10:42.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epcoh took: 0:10:45\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.28\n",
            "  Validation took: 0:00:20\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:11:05 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyBrXrdgIaZX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIDPxQ0eJNxn"
      },
      "source": [
        "## **英語のデータセットから訓練した効果を確認しましょう。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF9pLOtZwiO8"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "def sa_function(text):\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,   \n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 256,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "    input_ids = encoded_dict['input_ids'].to(device)\n",
        "    outputs = model(input_ids)\n",
        "    negative = outputs[0][0][0].item()\n",
        "    positive = outputs[0][0][1].item()\n",
        "    positive_p = np.exp(positive)/(np.exp(positive)+np.exp(negative))\n",
        "    negative_p = np.exp(negative)/(np.exp(positive)+np.exp(negative))\n",
        "    if positive_p>=negative_p:\n",
        "        result = {\"label\" : \"ポジティブ\",\"score\" : positive_p}\n",
        "    else:\n",
        "        result = {\"label\" : \"ネガティブ\",\"score\" : negative_p}\n",
        "            \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xJ_2N0i5xy4"
      },
      "source": [
        "* Yahoo映画サイトの[新着・作品ユーザーレビュー](https://movies.yahoo.co.jp/review/)　https://movies.yahoo.co.jp/review/\n",
        "\n",
        "* 映画.COMからの [映画レビュー人気作品ランキング](https://eiga.com/movie/review/ranking/)　https://eiga.com/movie/review/ranking/\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**以上二つ映画評価サイトからユーザーのコメントをコピーして、英語の微調整するモデルに試してください 。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydX1fOGL2AUG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c5f3562-a6a3-49f9-ac00-b0fd5bb6a0b0"
      },
      "source": [
        "test = \"\"\"\n",
        "映画でピエロと言えばホラー、ゾンビと言えばコメディというイメージが定着している中でまさかのゾンビ社会派ドラマ！？\n",
        "\n",
        "しかしこれがなかなか巧みな設定で、ゾンビも元々普通の人間、噛まれてゾンビ化してるけど死んでるわけじゃない、というわけで薬による治療も可能という世界のお話。\n",
        "ここでさらに回復後もゾンビの時の記憶は残っているという設定が心理描写の伸びしろを生み出していて、これらの新しい着眼点を存分に生かした娯楽作品になっています。\n",
        "\n",
        "私個人的には非感染者からの元ゾンビへの憎悪と迫害、そこからの反発を、現実の移民問題等と結びつけて何か伝えようという意図は感じなかったので社会派ドラマという見え方はしなかったです。\n",
        "\n",
        "みんな大好きなゾンビという古典クリーチャーを本質に沿いつつ深堀りできた事と、それが効果的に面白さにつながってる点にクリエーターの意識の高さを感じる秀作だと思いました。\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "sa_function(test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'ポジティブ', 'score': 0.7129202653290919}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iH0TUaXyztR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "669529c2-2139-4eec-afa3-5afd6a1b8f5a"
      },
      "source": [
        "test = \"\"\"\n",
        "レビューは低評価が多いのであまり期待しないで鑑賞。やはり低評価が頷ける内容でした(途中久々に睡魔が、、)。\n",
        "\n",
        "カンバーバッチ主演でこの出来はかなりもったいない！もう少しどうにかならなかったのか･･残念過ぎる。まずエジソンとウェスティングハウスのそれぞれの妻のエピソードは要らないだろう。完全に蛇足、時間の無駄。家族の話は全カットしてエジソン vs ウェスティングハウスのビジネス物に徹するべきだったのではないか？下手に家族の話を入れた事によって全体がぼやけた印象。特にウェスティングハウスの妻の\"私頭いいのよ\"的な話し方や態度が鼻についた。夫の威を借る妻･･。話の本筋に全く関係ない妻のエピソードに時間を割いてテスラの話をほとんど描かないのは理解不可能。テスラの天才ぶりが全く伝わってこず、ただ要領の悪い若者みたいになってしまって気の毒。\n",
        "\n",
        "専門的な用語等は言葉だけでなく、視覚での説明･解説があればかなり分かりやすくなっただろうと思う。エジソンとウェスティングハウスがずっといがみ合い･足の引っ張り合いをしているばかりで、成功して\"ヨッシャー👍！！\"みたいに歓喜で盛り上がりがる場面が無いので単調に感じる。これではヒットは難しく興収的にかなり厳しいだろう。\n",
        "もう少し工夫すれば良作になる題材だったと思うので本当にもったいない。\n",
        "\n",
        "確かに邦題もイマイチですね。そのまま\"電流戦争\"でもいい気がする。\n",
        "カンバーバッチならイミテーション･ゲームの方が断然おもしろかった。\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "sa_function(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'ネガティブ', 'score': 0.9829522138774879}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNO0RuCA-L6Z"
      },
      "source": [
        "中国語の映画評価を、この英語微調整するモデルで効果を確認しましょう。\n",
        "\n",
        "[豆瓣电影](https://movie.douban.com/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd4kjBzx-Kx0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a33456f-b46f-4162-fa4e-c52fc9169e97"
      },
      "source": [
        "#test = \"这个影片拍的非常差。\"\n",
        "#test = \"这个影片拍的很好。\"\n",
        "test = \"\"\"\n",
        "这个影片拍的有些故弄玄虚了。神神叨叨一个小时后揭露女主真实身份，随后便是男主急匆匆地一路反击，哪怕他前一分钟精神再恍惚，也能凭借强大的主角光环不再恍惚而硬扛到最后。反转太生硬，扯上邪教其实也并不是什么新鲜题材了。最后20多分钟太仓促，看的时候感觉像是导演在提示大家：搞快点，电影马上就结束了，别在乎什么逻辑了，能勉强圆回来就已经够意思啦。非常差。\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "sa_function(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'ネガティブ', 'score': 0.9841572709176958}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju-T6rB9-2LT"
      },
      "source": [
        "## 総論： 人工知能のニューラルネットワークと生物の脳\n",
        "\n",
        "\n",
        "一つ多言語NLPモデルは　人間のように　ニューラルネットワークが　知識を分かれば　多言語に渡って利用することが出来ます。\n",
        "\n",
        "* より多く外国語のデータリソースや知識から日本ビジネスを完成しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO-dQzs232D9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}